{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7636a5-8aad-46d6-87ba-4c97585afa65",
      "metadata": {
        "id": "5e7636a5-8aad-46d6-87ba-4c97585afa65"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a734c31-1688-4882-bb08-53f225d2aacf",
      "metadata": {
        "id": "0a734c31-1688-4882-bb08-53f225d2aacf"
      },
      "outputs": [],
      "source": [
        "# --- Step 1: Read in the Data Files ---\n",
        "# Update file paths as needed for your local environment.\n",
        "sales_df = pd.read_csv('./m5-forecasting-accuracy/sales_train_validation.csv')\n",
        "calendar_df = pd.read_csv('./m5-forecasting-accuracy/calendar.csv')\n",
        "sell_prices_df = pd.read_csv('./m5-forecasting-accuracy/sell_prices.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1238da43-c87e-4942-a4ed-e3c2c6687a97",
      "metadata": {
        "id": "1238da43-c87e-4942-a4ed-e3c2c6687a97"
      },
      "outputs": [],
      "source": [
        "# --- Step 2: Convert Sales Data from Wide to Long Format ---\n",
        "# The sales data is stored with one row per item and columns 'd_1' to 'd_N' representing dates.\n",
        "id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
        "value_vars = [col for col in sales_df.columns if col.startswith('d_')]\n",
        "sales_long = pd.melt(sales_df, id_vars=id_vars, value_vars=value_vars,\n",
        "                     var_name='d', value_name='sales')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eb15f3f-f04a-4d45-8d39-1856a22370e0",
      "metadata": {
        "id": "9eb15f3f-f04a-4d45-8d39-1856a22370e0"
      },
      "outputs": [],
      "source": [
        "# --- Step 3: Merge with Calendar Data ---\n",
        "# The calendar file has a column 'd' that aligns with our melted sales data.\n",
        "sales_long = sales_long.merge(calendar_df, on='d', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1756b5e-128e-412f-b8d4-257c300e1e8b",
      "metadata": {
        "id": "a1756b5e-128e-412f-b8d4-257c300e1e8b"
      },
      "outputs": [],
      "source": [
        "# --- Step 4: Merge with Sell Prices ---\n",
        "# Merge pricing information based on store, item, and the week (wm_yr_wk).\n",
        "sales_long = sales_long.merge(sell_prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7043bc47-60e7-48b0-9780-737f607d7afa",
      "metadata": {
        "id": "7043bc47-60e7-48b0-9780-737f607d7afa"
      },
      "outputs": [],
      "source": [
        "# --- Step 5: Preprocess Dates and Sort Data ---\n",
        "# Convert the 'date' column to datetime and sort by 'id' then date.\n",
        "sales_long['date'] = pd.to_datetime(sales_long['date'])\n",
        "sales_long.sort_values(by=['id', 'date'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbf545ab-47be-44e7-a367-981074b84135",
      "metadata": {
        "id": "fbf545ab-47be-44e7-a367-981074b84135"
      },
      "outputs": [],
      "source": [
        "# --- Step 6: Create Lag Features ---\n",
        "# These features capture previous days' sales, e.g., 7-day and 28-day lags.\n",
        "lag_days = [7, 28]\n",
        "for lag in lag_days:\n",
        "    sales_long[f'sales_lag_{lag}'] = sales_long.groupby('id')['sales'].shift(lag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16408e74-0f0f-415f-a945-dda23c92c60e",
      "metadata": {
        "id": "16408e74-0f0f-415f-a945-dda23c92c60e"
      },
      "outputs": [],
      "source": [
        "# --- Step 7: Create Rolling Statistics ---\n",
        "# Calculate rolling mean and standard deviation for given window sizes.\n",
        "rolling_windows = [7, 28]\n",
        "for window in rolling_windows:\n",
        "    # Use shift(1) to avoid using current day's sales in the rolling calculation.\n",
        "    sales_long[f'rolling_mean_{window}'] = sales_long.groupby('id')['sales'].transform(\n",
        "        lambda x: x.shift(1).rolling(window).mean())\n",
        "    sales_long[f'rolling_std_{window}'] = sales_long.groupby('id')['sales'].transform(\n",
        "        lambda x: x.shift(1).rolling(window).std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ffdc943-b39c-45b6-87a7-bbb0321fba4e",
      "metadata": {
        "id": "5ffdc943-b39c-45b6-87a7-bbb0321fba4e"
      },
      "outputs": [],
      "source": [
        "# --- Step 8: Generate Calendar-Based Features ---\n",
        "# Extract date parts that can capture seasonality: day of week, month, year, and weekend indicator.\n",
        "sales_long['dayofweek'] = sales_long['date'].dt.dayofweek\n",
        "sales_long['month'] = sales_long['date'].dt.month\n",
        "sales_long['year'] = sales_long['date'].dt.year\n",
        "sales_long['is_weekend'] = sales_long['dayofweek'].isin([5, 6]).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5910afba-a0fd-4b8f-b8cd-ec1a574da3ef",
      "metadata": {
        "id": "5910afba-a0fd-4b8f-b8cd-ec1a574da3ef"
      },
      "outputs": [],
      "source": [
        "# Optionally, flag special events using the calendar data (e.g., holidays or events)\n",
        "# For example, you could create a binary feature from an event column:\n",
        "if 'event_name_1' in sales_long.columns:\n",
        "    sales_long['is_event'] = sales_long['event_name_1'].notnull().astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "235fda06-d037-4d81-9b3e-b2339feb5dd6",
      "metadata": {
        "id": "235fda06-d037-4d81-9b3e-b2339feb5dd6",
        "outputId": "909497af-c9c5-4db4-e180-499f8f999f85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jakey\\AppData\\Local\\Temp\\ipykernel_6864\\1666045511.py:3: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
            "  sales_long['price_change_rate'] = sales_long.groupby('id')['sell_price'].pct_change()\n"
          ]
        }
      ],
      "source": [
        "# --- Step 9: Create Price-Based and Interaction Features ---\n",
        "# Example: Calculate the price change rate over time for each item.\n",
        "sales_long['price_change_rate'] = sales_long.groupby('id')['sell_price'].pct_change()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d81af8a7-92cc-4cad-8aa3-9eb087341174",
      "metadata": {
        "id": "d81af8a7-92cc-4cad-8aa3-9eb087341174"
      },
      "outputs": [],
      "source": [
        "# You could also create an interaction feature between promotional events and price\n",
        "sales_long['price_event_interaction'] = sales_long['sell_price'] * sales_long.get('is_event', 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4bce873-a243-4a53-8216-1e7064cc9e8d",
      "metadata": {
        "id": "b4bce873-a243-4a53-8216-1e7064cc9e8d",
        "outputId": "54454d48-9346-4eb0-cb5b-3269bd2809f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 id      item_id  dept_id cat_id store_id  \\\n",
            "1612    FOODS_1_001_CA_1_validation  FOODS_1_001  FOODS_1  FOODS     CA_1   \n",
            "32102   FOODS_1_001_CA_1_validation  FOODS_1_001  FOODS_1  FOODS     CA_1   \n",
            "62592   FOODS_1_001_CA_1_validation  FOODS_1_001  FOODS_1  FOODS     CA_1   \n",
            "93082   FOODS_1_001_CA_1_validation  FOODS_1_001  FOODS_1  FOODS     CA_1   \n",
            "123572  FOODS_1_001_CA_1_validation  FOODS_1_001  FOODS_1  FOODS     CA_1   \n",
            "\n",
            "       state_id    d  sales       date  wm_yr_wk  ... sales_lag_28  \\\n",
            "1612         CA  d_1      3 2011-01-29     11101  ...          NaN   \n",
            "32102        CA  d_2      0 2011-01-30     11101  ...          NaN   \n",
            "62592        CA  d_3      0 2011-01-31     11101  ...          NaN   \n",
            "93082        CA  d_4      1 2011-02-01     11101  ...          NaN   \n",
            "123572       CA  d_5      4 2011-02-02     11101  ...          NaN   \n",
            "\n",
            "        rolling_mean_7  rolling_std_7  rolling_mean_28 rolling_std_28  \\\n",
            "1612               NaN            NaN              NaN            NaN   \n",
            "32102              NaN            NaN              NaN            NaN   \n",
            "62592              NaN            NaN              NaN            NaN   \n",
            "93082              NaN            NaN              NaN            NaN   \n",
            "123572             NaN            NaN              NaN            NaN   \n",
            "\n",
            "       dayofweek is_weekend is_event  price_change_rate  \\\n",
            "1612           5          1        0                NaN   \n",
            "32102          6          1        0                0.0   \n",
            "62592          0          0        0                0.0   \n",
            "93082          1          0        0                0.0   \n",
            "123572         2          0        0                0.0   \n",
            "\n",
            "        price_event_interaction  \n",
            "1612                        0.0  \n",
            "32102                       0.0  \n",
            "62592                       0.0  \n",
            "93082                       0.0  \n",
            "123572                      0.0  \n",
            "\n",
            "[5 rows x 33 columns]\n"
          ]
        }
      ],
      "source": [
        "# Preview the enriched dataset\n",
        "print(sales_long.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f922b26d-3684-4ec2-9b65-7752089b31f0",
      "metadata": {
        "id": "f922b26d-3684-4ec2-9b65-7752089b31f0",
        "outputId": "0da525d8-bc38-4a98-f9e7-80d8875416c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd',\n",
              "       'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year',\n",
              "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
              "       'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'sales_lag_7',\n",
              "       'sales_lag_28', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_28',\n",
              "       'rolling_std_28', 'dayofweek', 'is_weekend', 'is_event',\n",
              "       'price_change_rate', 'price_event_interaction'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sales_long.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa49732f-a077-4b0f-8ca4-27e49b04f1ea",
      "metadata": {
        "id": "fa49732f-a077-4b0f-8ca4-27e49b04f1ea",
        "outputId": "19afd9ef-15f5-4b7b-8d78-0fd5dcbbd2a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lightgbm\n",
            "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\jakey\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in c:\\users\\jakey\\anaconda3\\lib\\site-packages (from lightgbm) (1.15.2)\n",
            "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.1/1.5 MB 825.8 kB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 0.2/1.5 MB 1.6 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 0.4/1.5 MB 2.5 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 0.5/1.5 MB 2.7 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 0.6/1.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 0.7/1.5 MB 2.7 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 0.9/1.5 MB 2.8 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 1.2/1.5 MB 3.0 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 1.4/1.5 MB 3.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.5/1.5 MB 3.0 MB/s eta 0:00:00\n",
            "Installing collected packages: lightgbm\n",
            "Successfully installed lightgbm-4.6.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da4b6d25-29a1-41ac-95e7-ec0185ba9d98",
      "metadata": {
        "id": "da4b6d25-29a1-41ac-95e7-ec0185ba9d98",
        "outputId": "f773e379-5c11-4cc6-88ce-ce8f451e6a53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.6.0\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "print(lgb.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63d775ac-119c-443d-ab8a-aeddf6f98541",
      "metadata": {
        "id": "63d775ac-119c-443d-ab8a-aeddf6f98541"
      },
      "outputs": [],
      "source": [
        "# Remove initial rows with NaN due to lag/rolling calculations\n",
        "df_model = sales_long.dropna(subset=['sales_lag_28'])\n",
        "\n",
        "# For this example, we choose a date cutoff.\n",
        "# Adjust the date thresholds based on your available data.\n",
        "train_df = df_model[df_model['date'] < '2016-04-25']\n",
        "valid_df = df_model[(df_model['date'] >= '2016-04-25') & (df_model['date'] <= '2016-05-22')]\n",
        "\n",
        "# Define feature columns (you can expand this list as you refine your features)\n",
        "features = [col for col in df_model.columns if col.startswith('sales_lag') or\n",
        "            col.startswith('rolling_') or col in ['dayofweek', 'month', 'is_weekend', 'price_change_rate']]\n",
        "target = 'sales'\n",
        "\n",
        "# Split into X and y\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target]\n",
        "X_valid = valid_df[features]\n",
        "y_valid = valid_df[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db3dcfb-ac0d-4e6e-ba97-026c414f0912",
      "metadata": {
        "id": "4db3dcfb-ac0d-4e6e-ba97-026c414f0912",
        "outputId": "07952237-1f6f-4ea2-b392-a501cc9520ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (57473650, 10)\n",
            "y_train shape: (57473650,)\n",
            "X_valid shape: (0, 10)\n",
            "y_valid shape: (0,)\n"
          ]
        }
      ],
      "source": [
        "# Debug prints: Verify that the data is non-empty and correctly shaped.\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_valid shape:\", X_valid.shape)\n",
        "print(\"y_valid shape:\", y_valid.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3241fa4a-cde2-4205-a6e0-6e5d71b6138d",
      "metadata": {
        "id": "3241fa4a-cde2-4205-a6e0-6e5d71b6138d",
        "outputId": "b7ea0eb6-cbb1-4ade-998a-6c1ec0cd0c4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min date in df_model: 2011-02-26 00:00:00\n",
            "Max date in df_model: 2016-04-24 00:00:00\n",
            "Train set date range: 2011-02-26 00:00:00 - 2016-03-26 00:00:00\n",
            "Validation set date range: 2016-03-27 00:00:00 - 2016-04-24 00:00:00\n",
            "X_train shape: (56589440, 10)\n",
            "y_train shape: (56589440,)\n",
            "X_valid shape: (884210, 10)\n",
            "y_valid shape: (884210,)\n"
          ]
        }
      ],
      "source": [
        "# Debug: Print available date range in df_model\n",
        "print(\"Min date in df_model:\", df_model['date'].min())\n",
        "print(\"Max date in df_model:\", df_model['date'].max())\n",
        "\n",
        "# For example, you might set the validation period as the last 28 days of available data.\n",
        "max_date = df_model['date'].max()\n",
        "train_df = df_model[df_model['date'] < (max_date - pd.Timedelta(days=28))]\n",
        "valid_df = df_model[df_model['date'] >= (max_date - pd.Timedelta(days=28))]\n",
        "\n",
        "# Confirm that both training and validation sets are non-empty.\n",
        "print(\"Train set date range:\", train_df['date'].min(), \"-\", train_df['date'].max())\n",
        "print(\"Validation set date range:\", valid_df['date'].min(), \"-\", valid_df['date'].max())\n",
        "\n",
        "print(\"X_train shape:\", train_df[features].shape)\n",
        "print(\"y_train shape:\", train_df[target].shape)\n",
        "print(\"X_valid shape:\", valid_df[features].shape)\n",
        "print(\"y_valid shape:\", valid_df[target].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a102479a-f9df-41b6-a8b6-e1b6fcd77e8b",
      "metadata": {
        "id": "a102479a-f9df-41b6-a8b6-e1b6fcd77e8b",
        "outputId": "8fc96aee-7908-4804-dfb7-e88ce91f3c4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[50]\ttraining's rmse: 2.14317\tvalid_1's rmse: 1.99707\n",
            "[100]\ttraining's rmse: 2.10865\tvalid_1's rmse: 1.97258\n",
            "[150]\ttraining's rmse: 2.09963\tvalid_1's rmse: 1.96996\n",
            "[200]\ttraining's rmse: 2.09376\tvalid_1's rmse: 1.96894\n",
            "[250]\ttraining's rmse: 2.08948\tvalid_1's rmse: 1.96838\n",
            "[300]\ttraining's rmse: 2.0858\tvalid_1's rmse: 1.96775\n",
            "[350]\ttraining's rmse: 2.0828\tvalid_1's rmse: 1.96757\n",
            "[400]\ttraining's rmse: 2.08022\tvalid_1's rmse: 1.96725\n",
            "[450]\ttraining's rmse: 2.0778\tvalid_1's rmse: 1.967\n",
            "Early stopping, best iteration is:\n",
            "[441]\ttraining's rmse: 2.07824\tvalid_1's rmse: 1.96694\n",
            "Best iteration: 441\n"
          ]
        }
      ],
      "source": [
        "# Create LightGBM Datasets from your training and validation data.\n",
        "lgb_train = lgb.Dataset(train_df[features], label=train_df[target])\n",
        "lgb_valid = lgb.Dataset(valid_df[features], label=valid_df[target])\n",
        "\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'verbose': -1  # to suppress internal logging\n",
        "}\n",
        "\n",
        "# Train the LightGBM model using callbacks for early stopping and logging.\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    lgb_train,\n",
        "    num_boost_round=1000,\n",
        "    valid_sets=[lgb_train, lgb_valid],\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(stopping_rounds=50),\n",
        "        lgb.log_evaluation(period=50)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Best iteration:\", model.best_iteration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d44014f4-81cc-4814-812d-cc7ed49014c6",
      "metadata": {
        "id": "d44014f4-81cc-4814-812d-cc7ed49014c6",
        "outputId": "ef95ff2a-3a7c-4daf-f058-897b47a2a6c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<lightgbm.basic.Booster at 0x27056009280>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model to a file.\n",
        "model.save_model('m5_trained_model.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bfc7830-034e-43a9-8491-28c76de17eed",
      "metadata": {
        "id": "4bfc7830-034e-43a9-8491-28c76de17eed",
        "outputId": "75644c61-8182-46fe-9cea-be26c81fe31d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             feature  importance\n",
            "3     rolling_mean_7        2874\n",
            "5    rolling_mean_28        2007\n",
            "2       sales_lag_28        1887\n",
            "1        sales_lag_7        1812\n",
            "7          dayofweek        1503\n",
            "0              month        1400\n",
            "4      rolling_std_7         979\n",
            "6     rolling_std_28         419\n",
            "9  price_change_rate         320\n",
            "8         is_weekend          29\n"
          ]
        }
      ],
      "source": [
        "# Extract feature importances.\n",
        "importances = model.feature_importance()\n",
        "feature_names = features  # The feature list used during training.\n",
        "\n",
        "# Combine and display in a DataFrame\n",
        "feat_imp = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': importances\n",
        "}).sort_values(by='importance', ascending=False)\n",
        "\n",
        "print(feat_imp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53c4b44e-6d87-48c8-8413-c3f5d88d0feb",
      "metadata": {
        "id": "53c4b44e-6d87-48c8-8413-c3f5d88d0feb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e8d3be-1a27-4a12-825f-998e7afb16c2",
      "metadata": {
        "id": "b8e8d3be-1a27-4a12-825f-998e7afb16c2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41493ede-28ab-42fb-9d71-d69c47c672d2",
      "metadata": {
        "id": "41493ede-28ab-42fb-9d71-d69c47c672d2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "625b412b-2e89-4fdc-a0e4-92d6cf0aaa90",
      "metadata": {
        "id": "625b412b-2e89-4fdc-a0e4-92d6cf0aaa90",
        "outputId": "4ceada29-6516-4bb1-be0b-a49947f5b87a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forecasted Sales for id: FOODS_1_001_CA_1_validation\n",
            "         date  predicted_sales\n",
            "0  2016-04-25         1.108243\n",
            "1  2016-04-26         0.950177\n",
            "2  2016-04-27         0.950177\n",
            "3  2016-04-28         0.966403\n",
            "4  2016-04-29         1.134161\n",
            "5  2016-04-30         1.169028\n",
            "6  2016-05-01         1.198295\n",
            "7  2016-05-02         1.111982\n",
            "8  2016-05-03         0.900066\n",
            "9  2016-05-04         0.933213\n",
            "10 2016-05-05         0.853214\n",
            "11 2016-05-06         0.930345\n",
            "12 2016-05-07         1.092960\n",
            "13 2016-05-08         1.148746\n",
            "14 2016-05-09         0.940508\n",
            "15 2016-05-10         0.850561\n",
            "16 2016-05-11         0.846066\n",
            "17 2016-05-12         0.846066\n",
            "18 2016-05-13         0.903890\n",
            "19 2016-05-14         1.114828\n",
            "20 2016-05-15         1.052194\n",
            "21 2016-05-16         1.001207\n",
            "22 2016-05-17         0.775999\n",
            "23 2016-05-18         0.775999\n",
            "24 2016-05-19         0.705013\n",
            "25 2016-05-20         0.755930\n",
            "26 2016-05-21         0.882916\n",
            "27 2016-05-22         0.882916\n"
          ]
        }
      ],
      "source": [
        "# Choose a sample time series 'id'\n",
        "sample_id = df_model['id'].unique()[0]\n",
        "sample_series = df_model[df_model['id'] == sample_id].copy().sort_values('date')\n",
        "last_known_date = sample_series['date'].max()\n",
        "forecast_dates = pd.date_range(start=last_known_date + pd.Timedelta(days=1), periods=28)\n",
        "\n",
        "# Create a temporary DataFrame for recursive forecasting.\n",
        "temp_series = sample_series.copy()\n",
        "forecast_preds = []\n",
        "\n",
        "for forecast_date in forecast_dates:\n",
        "    new_features = {}\n",
        "    new_features['date'] = forecast_date\n",
        "\n",
        "    # Generate lag features for forecast_date.\n",
        "    for lag in [7, 28]:\n",
        "        lag_date = forecast_date - pd.Timedelta(days=lag)\n",
        "        lag_value = temp_series.loc[temp_series['date'] == lag_date, 'sales']\n",
        "        new_features[f'sales_lag_{lag}'] = lag_value.values[0] if not lag_value.empty else np.nan\n",
        "\n",
        "    # Compute rolling statistics.\n",
        "    for window in [7, 28]:\n",
        "        window_start = forecast_date - pd.Timedelta(days=window)\n",
        "        window_end = forecast_date - pd.Timedelta(days=1)\n",
        "        window_sales = temp_series[(temp_series['date'] >= window_start) & (temp_series['date'] <= window_end)]['sales']\n",
        "        new_features[f'rolling_mean_{window}'] = window_sales.mean() if len(window_sales) > 0 else np.nan\n",
        "        new_features[f'rolling_std_{window}'] = window_sales.std() if len(window_sales) > 0 else np.nan\n",
        "\n",
        "    # Calendar features.\n",
        "    new_features['dayofweek'] = forecast_date.dayofweek\n",
        "    new_features['month'] = forecast_date.month\n",
        "    new_features['is_weekend'] = int(forecast_date.dayofweek in [5, 6])\n",
        "\n",
        "    # Set default for price_change_rate (customize as needed).\n",
        "    new_features['price_change_rate'] = 0\n",
        "\n",
        "    # Create a DataFrame row for prediction.\n",
        "    new_row_df = pd.DataFrame([new_features])\n",
        "    X_new = new_row_df[features]\n",
        "    pred_sales = model.predict(X_new)[0]\n",
        "    forecast_preds.append(pred_sales)\n",
        "\n",
        "    # Append the new forecast to update features recursively.\n",
        "    new_features['sales'] = pred_sales\n",
        "    new_features['id'] = sample_id\n",
        "    temp_series = pd.concat([temp_series, pd.DataFrame([new_features])], ignore_index=True)\n",
        "\n",
        "# Compile forecasts for display.\n",
        "forecast_df = pd.DataFrame({\n",
        "    'date': forecast_dates,\n",
        "    'predicted_sales': forecast_preds\n",
        "})\n",
        "print(\"Forecasted Sales for id:\", sample_id)\n",
        "print(forecast_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edfd466d-ddc8-4099-8c84-1b4c5304ad8e",
      "metadata": {
        "id": "edfd466d-ddc8-4099-8c84-1b4c5304ad8e",
        "outputId": "0daeb68f-515d-40a2-d158-fc12a62037e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[50]\ttraining's rmse: 2.17529\tvalid_1's rmse: 2.00651\n",
            "[100]\ttraining's rmse: 2.12749\tvalid_1's rmse: 1.97321\n",
            "[150]\ttraining's rmse: 2.11714\tvalid_1's rmse: 1.9695\n",
            "[200]\ttraining's rmse: 2.11025\tvalid_1's rmse: 1.96779\n",
            "[250]\ttraining's rmse: 2.1056\tvalid_1's rmse: 1.96693\n",
            "[300]\ttraining's rmse: 2.10187\tvalid_1's rmse: 1.96633\n",
            "[350]\ttraining's rmse: 2.09878\tvalid_1's rmse: 1.96592\n",
            "[400]\ttraining's rmse: 2.09576\tvalid_1's rmse: 1.96564\n",
            "[450]\ttraining's rmse: 2.09341\tvalid_1's rmse: 1.96556\n",
            "[500]\ttraining's rmse: 2.09133\tvalid_1's rmse: 1.9655\n",
            "[550]\ttraining's rmse: 2.08912\tvalid_1's rmse: 1.96537\n",
            "[600]\ttraining's rmse: 2.08702\tvalid_1's rmse: 1.96522\n",
            "[650]\ttraining's rmse: 2.08544\tvalid_1's rmse: 1.96516\n",
            "Early stopping, best iteration is:\n",
            "[647]\ttraining's rmse: 2.08558\tvalid_1's rmse: 1.96514\n",
            "Best iteration for Tweedie model: 647\n"
          ]
        }
      ],
      "source": [
        "# Define parameters for the Tweedie model.\n",
        "params_tweedie = {\n",
        "    'objective': 'tweedie',\n",
        "    'tweedie_variance_power': 1.1,  # Adjust between 1.0 (Poisson) and 2.0 (Gamma) as needed.\n",
        "    'metric': 'rmse',               # We use RMSE here for monitoring (LightGBM does not natively evaluate WRMSSE).\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'verbose': -1\n",
        "}\n",
        "\n",
        "# Create LightGBM Datasets from the training and validation sets.\n",
        "lgb_train = lgb.Dataset(train_df[features], label=train_df[target])\n",
        "lgb_valid = lgb.Dataset(valid_df[features], label=valid_df[target])\n",
        "\n",
        "# Train the model using callbacks for early stopping and logging.\n",
        "model_tweedie = lgb.train(\n",
        "    params_tweedie,\n",
        "    lgb_train,\n",
        "    num_boost_round=1000,\n",
        "    valid_sets=[lgb_train, lgb_valid],\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(stopping_rounds=50),\n",
        "        lgb.log_evaluation(period=50)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Best iteration for Tweedie model:\", model_tweedie.best_iteration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6c4612-b13b-48e4-a4be-4dd5fa0c54c6",
      "metadata": {
        "id": "4d6c4612-b13b-48e4-a4be-4dd5fa0c54c6"
      },
      "outputs": [],
      "source": [
        "# Get predictions from the Tweedie model on the validation set.\n",
        "val_preds_tweedie = model_tweedie.predict(valid_df[features])\n",
        "\n",
        "# Attach the predictions to the validation DataFrame.\n",
        "valid_df_with_preds = valid_df.copy()\n",
        "valid_df_with_preds['predicted'] = val_preds_tweedie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a658eaa-ef23-48c8-ae6c-17cb2bf736b0",
      "metadata": {
        "id": "0a658eaa-ef23-48c8-ae6c-17cb2bf736b0"
      },
      "outputs": [],
      "source": [
        "def simple_wrmsse(train_df, valid_df):\n",
        "    \"\"\"\n",
        "    Computes a simplified WRMSSE score.\n",
        "\n",
        "    Parameters:\n",
        "        train_df : DataFrame\n",
        "            Training data including 'id', 'date', and 'sales'.\n",
        "        valid_df : DataFrame\n",
        "            Validation data including 'id', 'date', 'sales', and 'predicted'.\n",
        "\n",
        "    Returns:\n",
        "        overall_wrmsse : float\n",
        "            The weighted RMSSE computed across series.\n",
        "    \"\"\"\n",
        "    series_ids = valid_df['id'].unique()\n",
        "    weighted_errors = []\n",
        "    weights = []\n",
        "\n",
        "    for sid in series_ids:\n",
        "        # Get training sales history for this series.\n",
        "        train_sales = train_df[train_df['id'] == sid].sort_values('date')['sales'].values\n",
        "        # Get actual validation sales.\n",
        "        valid_sales = valid_df[valid_df['id'] == sid].sort_values('date')['sales'].values\n",
        "        # Get predictions for this series.\n",
        "        pred_sales = valid_df[valid_df['id'] == sid].sort_values('date')['predicted'].values\n",
        "\n",
        "        # Compute the scaling factor: RMS of the differences in training sales.\n",
        "        if len(train_sales) > 1:\n",
        "            scale = np.sqrt(np.mean(np.diff(train_sales) ** 2))\n",
        "        else:\n",
        "            scale = 1.0\n",
        "\n",
        "        # Compute RMSE for the validation period for this series.\n",
        "        rmse = np.sqrt(np.mean((valid_sales - pred_sales) ** 2))\n",
        "\n",
        "        # Calculate the series-specific RMSSE.\n",
        "        series_error = rmse / scale if scale > 0 else rmse\n",
        "\n",
        "        # Weight for the series: Here we use the sum of training sales.\n",
        "        series_weight = train_sales.sum()\n",
        "\n",
        "        weighted_errors.append(series_error ** 2 * series_weight)\n",
        "        weights.append(series_weight)\n",
        "\n",
        "    overall_wrmsse = np.sqrt(np.sum(weighted_errors) / np.sum(weights))\n",
        "    return overall_wrmsse\n",
        "\n",
        "# Compute the simplified WRMSSE score.\n",
        "wrmsse_score = simple_wrmsse(train_df, valid_df_with_preds)\n",
        "print(\"Simplified WRMSSE score for Tweedie model:\", wrmsse_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc81b519-ae9d-41e0-b626-da8da4f0bc2e",
      "metadata": {
        "id": "fc81b519-ae9d-41e0-b626-da8da4f0bc2e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}